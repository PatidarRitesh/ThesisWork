<?xml version='1.0' encoding='utf-8'?>
<ns0:document xmlns:ns0="http://dlmf.nist.gov/LaTeXML" class="ltx_authors_1line">
  <ns0:resource src="LaTeXML.css" type="text/css" />
  <ns0:resource src="ltx-article.css" type="text/css" />
  <ns0:title>HinGE: A Dataset for Generation and Evaluation of Code-Mixed Hinglish Text</ns0:title>
  <ns0:creator role="author">
    <ns0:personname>Vivek Srivastava <ns0:break />TCS Research<ns0:break />Pune, Maharashtra, India <ns0:break /><ns0:text font="typewriter">srivastava.vivek2@tcs.com</ns0:text> <ns0:break /><ns0:ERROR class="undefined">\And</ns0:ERROR>Mayank Singh <ns0:break />IIT Gandhinagar<ns0:break />Gandhinagar, Gujarat, India <ns0:break /><ns0:text font="typewriter">singh.mayank@iitgn.ac.in</ns0:text> <ns0:break /></ns0:personname>
  </ns0:creator>
  <ns0:abstract name="Abstract">
    <ns0:p>Text generation is a highly active area of research in the computational linguistic community. The evaluation of the generated text is a challenging task and multiple theories and metrics have been proposed over the years. Unfortunately, text generation and evaluation are relatively understudied due to the scarcity of high-quality resources in code-mixed languages where the words and phrases from multiple languages are mixed in a single utterance of text and speech. To address this challenge, we present a corpus (<ns0:text font="italic">HinGE</ns0:text>) for a widely popular code-mixed language Hinglish (code-mixing of Hindi and English languages). <ns0:text font="italic">HinGE</ns0:text> has Hinglish sentences generated by humans as well as two rule-based algorithms corresponding to the parallel Hindi-English sentences. In addition, we demonstrate the inefficacy of widely-used evaluation metrics on the code-mixed data. The <ns0:text font="italic">HinGE</ns0:text> dataset will facilitate the progress of natural language generation research in code-mixed languages.</ns0:p>
  </ns0:abstract>
  <ns0:section inlist="toc" xml:id="S1">
    <ns0:tags>
      <ns0:tag>1</ns0:tag>
      <ns0:tag role="refnum">1</ns0:tag>
      <ns0:tag role="typerefnum">§1</ns0:tag>
    </ns0:tags>
    <ns0:title><ns0:tag close=" ">1</ns0:tag>Introduction</ns0:title>
    <ns0:para xml:id="S1.p1">
      <ns0:p>Code-mixing is the mixing of two or more languages in a single utterance of speech or text. A commonly observed communication pattern for a multilingual speaker is to mix words and phrases from multiple languages. Code-mixing is widespread across various language pairs, such as Spanish-English, Hindi-English, and Bengali-English. Recently, we observe a boom in the availability of code-mixed data with the inflation of the social media platforms such as Twitter and Facebook.</ns0:p>
    </ns0:para>
    <ns0:para xml:id="S1.p2">
      <ns0:p>In past, we witness magnitude of work to address standard code-mixing natural language understanding (NLU) tasks such as language identification <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="shekhar2020language,singh2018language,ramanarayanan2019automatic" separator="," yyseparator="," />]</ns0:cite>, POS tagging <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="singh2018twitter,vyas2014pos" separator="," yyseparator="," />]</ns0:cite>, named entity recognition  <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="singh2018language" separator="," yyseparator="," />]</ns0:cite>, and dependency parsing <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="zhang2019cross" separator="," yyseparator="," />]</ns0:cite> along with sentence classification tasks like sentiment analysis <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="patwa2020semeval,joshi2016towards" separator="," yyseparator="," />]</ns0:cite>, stance detection <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="10.1145/3371158.3371226" separator="," yyseparator="," />]</ns0:cite>, and sarcasm detection <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="swami2018corpus" separator="," yyseparator="," />]</ns0:cite>. Unlike code-mixed NLU, natural language generation (NLG) of code-mixed text is highly understudied. Resource scarcity adds to the challenge of building efficient solutions for code-mixed NLG tasks. Evaluation of the code-mixed NLG tasks also lacks standalone resources, theories, and metrics.</ns0:p>
    </ns0:para>
    <ns0:para xml:id="S1.p3">
      <ns0:p>Recently, we observe a growing interest in the code-mixed text generation task. To generate the code-mixed data various techniques have been employed such as matrix frame language theory <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="Lee2019,gupta-etal-2020-semi,dhruval_GCM_dependency_2021" separator="," yyseparator="," />]</ns0:cite>, equivalent constraint theory <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="pratapa-etal-2018-language" separator="," yyseparator="," />]</ns0:cite>, pointer-generator network <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="2018arXiv181010254I,winata-etal-2019-code,gupta-etal-2020-semi" separator="," yyseparator="," />]</ns0:cite>, Generative Adversarial Networks (GANs) <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="Gao_Bert_GAN_2019" separator="," yyseparator="," />]</ns0:cite>, etc. The majority of the available datasets <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="rijhwani-etal-2017-estimating,solorio-etal-2014-overview,patro-etal-2017-english" separator="," yyseparator="," />]</ns0:cite> employed in code-mixed NLG contains noisy code-mixed text collected from social media platforms such as Twitter. These datasets also lack the sanity check for the quality of sentences, making the systems developed on these datasets vulnerable to real-world applicability.
To address the challenge of scarcity of high-quality resources for the code-mixed NLG tasks, we propose <ns0:text font="italic">HinGE</ns0:text> dataset that will facilitate the community to build robust systems. The dataset contains sentences generated by humans as well as two rule-based algorithms. In Table <ns0:ref labelref="LABEL:tab:comparison" />, we compare <ns0:text font="italic">HinGE</ns0:text> with three other baseline datasets that can be used in the Hinglish code-mixed text generation and evaluation task.</ns0:p>
    </ns0:para>
    <ns0:table inlist="lot" labels="LABEL:tab:comparison" placement="!tbh" xml:id="S1.T1">
      <ns0:tags>
        <ns0:tag>Table 1</ns0:tag>
        <ns0:tag role="refnum">1</ns0:tag>
        <ns0:tag role="typerefnum">Table 1</ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="ltx_centering undefined">\resizebox</ns0:ERROR>
      <ns0:p align="center">!
<ns0:tabular vattach="middle">
          <ns0:tbody>
            <ns0:tr>
              <ns0:td align="center" border="l r t"><ns0:text font="bold">Dataset characteristics</ns0:text></ns0:td>
              <ns0:td align="center" border="r t"><ns0:ERROR class="undefined">\newcite</ns0:ERROR>banerjee2018dataset</ns0:td>
              <ns0:td align="center" border="r t"><ns0:ERROR class="undefined">\newcite</ns0:ERROR>srivastava2020phinc</ns0:td>
              <ns0:td align="center" border="r t"><ns0:ERROR class="undefined">\newcite</ns0:ERROR>gupta-etal-2020-semi</ns0:td>
              <ns0:td align="center" border="r t"><ns0:text font="bold italic">HinGE</ns0:text></ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="l r t">Parallel source sentences</ns0:td>
              <ns0:td align="center" border="r t">Only ES</ns0:td>
              <ns0:td align="center" border="r t">Only ES</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="l r t">Human-generated code-mixed sentences</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="l r t"><ns0:tabular vattach="middle">
                  <ns0:tr>
                    <ns0:td align="center">Multiple human-generated code-mixed</ns0:td>
                  </ns0:tr>
                  <ns0:tr>
                    <ns0:td align="center">sentences for a parallel sentence</ns0:td>
                  </ns0:tr>
                </ns0:tabular></ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="l r t">Machine-generated code-mixed sentences</ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="l r t"><ns0:tabular vattach="middle">
                  <ns0:tr>
                    <ns0:td align="center">Multiple machine-generated code-mixed</ns0:td>
                  </ns0:tr>
                  <ns0:tr>
                    <ns0:td align="center">sentences for a parallel sentence</ns0:td>
                  </ns0:tr>
                </ns0:tabular></ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="l r t"><ns0:tabular vattach="middle">
                  <ns0:tr>
                    <ns0:td align="center">Human ratings for the quality of</ns0:td>
                  </ns0:tr>
                  <ns0:tr>
                    <ns0:td align="center">generated code-mixed sentences</ns0:td>
                  </ns0:tr>
                </ns0:tabular></ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✗</ns0:td>
              <ns0:td align="center" border="r t">✓</ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="b l r t">Dataset size</ns0:td>
              <ns0:td align="center" border="b r t"><ns0:tabular vattach="middle">
                  <ns0:tr>
                    <ns0:td align="center">6733 UEU,</ns0:td>
                  </ns0:tr>
                  <ns0:tr>
                    <ns0:td align="center">6549 UHU</ns0:td>
                  </ns0:tr>
                </ns0:tabular></ns0:td>
              <ns0:td align="center" border="b r t">13,738</ns0:td>
              <ns0:td align="center" border="b r t"><ns0:tabular vattach="middle">
                  <ns0:tr>
                    <ns0:td align="center">1,561,840 PS,</ns0:td>
                  </ns0:tr>
                  <ns0:tr>
                    <ns0:td align="center">252,330 MGHS</ns0:td>
                  </ns0:tr>
                </ns0:tabular></ns0:td>
              <ns0:td align="center" border="b r t"><ns0:tabular vattach="middle">
                  <ns0:tr>
                    <ns0:td align="center">1,976 PS,</ns0:td>
                  </ns0:tr>
                  <ns0:tr>
                    <ns0:td align="center">4,803 HGHS,</ns0:td>
                  </ns0:tr>
                  <ns0:tr>
                    <ns0:td align="center">3,952 MGHS</ns0:td>
                  </ns0:tr>
                </ns0:tabular></ns0:td>
            </ns0:tr>
          </ns0:tbody>
        </ns0:tabular></ns0:p>
      <ns0:toccaption class="ltx_centering"><ns0:tag close=" ">1</ns0:tag>Comparison between the various datasets available for the Hinglish NLG tasks. UEU: Unique English Utterance, UHU: Unique Hinglish Utterance, ES: English Sentences, PS: Parallel Sentences, HGHS: Human-Generated Hinglish Sentences, MGHS: Machine-Generated Hinglish Sentences.</ns0:toccaption>
      <ns0:caption class="ltx_centering"><ns0:tag close=": ">Table 1</ns0:tag>Comparison between the various datasets available for the Hinglish NLG tasks. UEU: Unique English Utterance, UHU: Unique Hinglish Utterance, ES: English Sentences, PS: Parallel Sentences, HGHS: Human-Generated Hinglish Sentences, MGHS: Machine-Generated Hinglish Sentences.</ns0:caption>
    </ns0:table>
    <ns0:para xml:id="S1.p4">
      <ns0:p>In addition to the code-mixed NLG, the evaluation of the generated code-mixed text is a challenging task. The widely popular metrics for monolingual languages fail to capture the linguistic diversity present in the code-mixed data, such as spelling variation and complex sentence structuring. The quality ratings of the sentences generated by the rule-based algorithms in <ns0:text font="italic">HinGE</ns0:text> dataset will help to develop the metrics and theories for evaluating the code-mixed NLG tasks.
Our main contributions are:</ns0:p>
      <ns0:itemize xml:id="S1.I1">
        <ns0:item xml:id="S1.I1.i1">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">1st item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S1.I1.i1.p1">
            <ns0:p>We create high-quality human-generated code-mixed Hinglish sentences corresponding to the parallel Hindi-English sentences. Each pair of parallel sentences has at least two human-generated Hinglish sentences.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S1.I1.i2">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">2nd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S1.I1.i2.p1">
            <ns0:p>In addition to the human-generated code-mixed sentences, we propose two rule-based algorithms to generate the Hinglish sentences.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S1.I1.i3">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">3rd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S1.I1.i3.p1">
            <ns0:p>We demonstrate the inefficacy of five widely popular metrics for the NLG task with the code-mixed text.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S1.I1.i4">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">4th item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S1.I1.i4.p1">
            <ns0:p>To develop efficient metrics for the code-mixed NLG tasks, we provide the human ratings corresponding to the code-mixed sentences generated by the rule-based algorithms.</ns0:p>
          </ns0:para>
        </ns0:item>
      </ns0:itemize>
    </ns0:para>
  </ns0:section>
  <ns0:section inlist="toc" labels="LABEL:sec:dataset" xml:id="S2">
    <ns0:tags>
      <ns0:tag>2</ns0:tag>
      <ns0:tag role="refnum">2</ns0:tag>
      <ns0:tag role="typerefnum">§2</ns0:tag>
    </ns0:tags>
    <ns0:title><ns0:tag close=" ">2</ns0:tag>Human-Generated Hinglish Text</ns0:title>
    <ns0:para xml:id="S2.p1">
      <ns0:p>The scarcity of high-quality code-mixed datasets limits current research in various NLU tasks such as text generation and summarization. To address this challenge, we create a human-generated corpus of Hinglish sentences corresponding to parallel monolingual English and Hindi sentences. We use the IIT Bombay English-Hindi Parallel Corpus (hereafter <ns0:text font="italic">‘IIT-B corpus’</ns0:text>) <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="kunchukuttan2018iit" separator="," yyseparator="," />]</ns0:cite>. The IIT-B corpus has 1,561,840 parallel sentence pairs in English and Hindi. The English sentences are written in the Roman script, and the Hindi sentences are written in the Devanagari script. We randomly select 5,000 sentence pairs, in which the number of tokens in both the sentences is more than five to create a human-generated parallel corpus.</ns0:p>
    </ns0:para>
    <ns0:para xml:id="S2.p2">
      <ns0:p>To create the gold-standard dataset, we employ five human annotators. Each annotator has expert-level proficiency in writing, speaking, and understanding English and Hindi languages. The objective of the annotation is to generate at least two unique Hinglish code-mixed sentences corresponding to the parallel English and Hindi sentence pairs. The annotators can also generate more than two code-mixed sentences for each sentence pair. We shuffle, pre-process, and share the sentence pairs with the annotators to generate the corresponding Hinglish sentences. A single annotator annotates each sentence pair.</ns0:p>
    </ns0:para>
    <ns0:para xml:id="S2.p3">
      <ns0:p>We assign 1,000 unique sentence pairs to each annotator with the following annotation guidelines:</ns0:p>
      <ns0:itemize xml:id="S2.I1">
        <ns0:item xml:id="S2.I1.i1">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">1st item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S2.I1.i1.p1">
            <ns0:p>The Hinglish sentence should be written in Roman script.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S2.I1.i2">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">2nd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S2.I1.i2.p1">
            <ns0:p>The Hinglish sentence should have words from both the languages, i.e., English and Hindi.
</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S2.I1.i3">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">3rd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S2.I1.i3.p1">
            <ns0:p>Avoid using new words, wherever possible, that are not present in both the sentences.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S2.I1.i4">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">4th item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S2.I1.i4.p1">
            <ns0:p>If the source sentences are not the translation of each other, mark the sentence pair as “#”.</ns0:p>
          </ns0:para>
        </ns0:item>
      </ns0:itemize>
    </ns0:para>
    <ns0:para xml:id="S2.p4">
      <ns0:p>Post annotation, we remove the sentence pairs marked as “#” or are missing an annotation. Note that due to the complexity of generating code-mixed sentences, such as the inability to identify two unique Hinglish sentences, complex sentence structuring, and usage of difficult words in monolingual sentences, annotators do not provide two unique sentences Hinglish sentences for each monolingual sentence pair. We obtain 1,978 sentence pairs with two or more unique Hinglish sentences.
On average, 2.5 code-mixed sentences are created for each Hindi-English sentence pair. Figure <ns0:ref labelref="LABEL:fig:example_cms" /> shows an example of two code-mixed sentences generated by the annotator for a given sentence pair.</ns0:p>
    </ns0:para>
    <ns0:para class="ltx_noindent" xml:id="S2.p5">
      <ns0:p><ns0:text font="bold">Qualitative evaluation of the human-generated Hinglish text:</ns0:text>
To qualitatively evaluate the generated sentences, we adapt the evaluation strategy described in <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="srivastava2021challenges" separator="," yyseparator="," />]</ns0:cite>. We randomly sample 100 Hinglish sentences generated by humans along with the source parallel monolingual English and Hindi sentences. We employ two human annotators<ns0:note mark="1" role="footnote" xml:id="footnote1"><ns0:tags>
            <ns0:tag>1</ns0:tag>
            <ns0:tag role="refnum">1</ns0:tag>
            <ns0:tag role="typerefnum">footnote 1</ns0:tag>
          </ns0:tags>different from the human annotators who generate the Hinglish sentences.</ns0:note> for the qualitative evaluation. We ask the annotators to rate each Hinglish sentence on two metrics:</ns0:p>
      <ns0:itemize xml:id="S2.I2">
        <ns0:item xml:id="S2.I2.i1">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">1st item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S2.I2.i1.p1">
            <ns0:p><ns0:text font="bold">Degree of code-mixing (DCM)</ns0:text>: The score can vary between 0 to 10. A DCM score of 0 corresponds to the monolingual sentence with no code-mixing, whereas the DCM score of 10 suggests a high degree of code-mixing.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S2.I2.i2">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">2nd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S2.I2.i2.p1">
            <ns0:p><ns0:text font="bold">Readability (RA)</ns0:text>: RA score can vary between 0 to 10. A completely unreadable sentence due to many spelling mistakes, no sentence structuring, or meaning yields a RA score of 0. A RA score of 10 suggests a highly readable sentence with clear semantics and easy-to-read words.</ns0:p>
          </ns0:para>
        </ns0:item>
      </ns0:itemize>
    </ns0:para>
    <ns0:para xml:id="S2.p6">
      <ns0:p>The average DCM scores by the two human annotators are 8.72 and 8.65. The average RA scores are 8.65 and 8.37. The high average scores demonstrate good quality code-mixed sentence generation. Table <ns0:ref labelref="LABEL:tab:example_rating" /> shows example ratings provided by the two human annotators to the five Hinglish sentences.</ns0:p>
    </ns0:para>
    <ns0:figure inlist="lof" labels="LABEL:fig:example_cms" placement="!tbh" xml:id="S2.F1">
      <ns0:tags>
        <ns0:tag>Figure 1</ns0:tag>
        <ns0:tag role="refnum">1</ns0:tag>
        <ns0:tag role="typerefnum">Figure 1</ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="ltx_centering undefined">\includegraphics</ns0:ERROR>
      <ns0:p align="center">[width=1.0]example.png</ns0:p>
      <ns0:toccaption class="ltx_centering"><ns0:tag close=" ">1</ns0:tag>Example of the code-mixed sentences generated by the annotator for an <ns0:text color="#FF8000">English</ns0:text>-<ns0:text color="#0000FF">Hindi</ns0:text> sentence pair.</ns0:toccaption>
      <ns0:caption class="ltx_centering"><ns0:tag close=": ">Figure 1</ns0:tag>Example of the code-mixed sentences generated by the annotator for an <ns0:text color="#FF8000">English</ns0:text>-<ns0:text color="#0000FF">Hindi</ns0:text> sentence pair.</ns0:caption>
    </ns0:figure>
    <ns0:table inlist="lot" labels="LABEL:tab:example_rating" placement="!tbh" xml:id="S2.T2">
      <ns0:tags>
        <ns0:tag><ns0:text fontsize="90%">Table 2</ns0:text></ns0:tag>
        <ns0:tag role="refnum"><ns0:text fontsize="90%">2</ns0:text></ns0:tag>
        <ns0:tag role="typerefnum"><ns0:text fontsize="90%">Table 2</ns0:text></ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="ltx_centering undefined">\resizebox</ns0:ERROR>
      <ns0:p align="center"><ns0:text fontsize="90%">!
<ns0:tabular vattach="middle">
            <ns0:tbody>
              <ns0:tr>
                <ns0:td align="center" border="l r t" rowspan="2"><ns0:text font="bold">Hinglish sentences</ns0:text></ns0:td>
                <ns0:td align="center" border="r t" colspan="2"><ns0:text font="bold">Human 1</ns0:text></ns0:td>
                <ns0:td align="center" border="r t" colspan="2"><ns0:text font="bold">Human 2</ns0:text></ns0:td>
              </ns0:tr>
              <ns0:tr>
                <ns0:td align="center" border="r t"><ns0:text font="bold">DCM</ns0:text></ns0:td>
                <ns0:td align="center" border="r t"><ns0:text font="bold">RA</ns0:text></ns0:td>
                <ns0:td align="center" border="r t"><ns0:text font="bold">DCM</ns0:text></ns0:td>
                <ns0:td align="center" border="r t"><ns0:text font="bold">RA</ns0:text></ns0:td>
              </ns0:tr>
              <ns0:tr>
                <ns0:td align="center" border="l r t"><ns0:text color="#FF8000">Media</ns0:text> <ns0:text color="#0000FF">ke</ns0:text> <ns0:text color="#FF8000">exposure</ns0:text> <ns0:text color="#0000FF">ke aadhar par</ns0:text> <ns0:text color="#FF8000">Indian</ns0:text> <ns0:text color="#0000FF">rajyo ki</ns0:text> <ns0:text color="#FF8000">rankings</ns0:text></ns0:td>
                <ns0:td align="center" border="r t">10</ns0:td>
                <ns0:td align="center" border="r t">10</ns0:td>
                <ns0:td align="center" border="r t">8</ns0:td>
                <ns0:td align="center" border="r t">8</ns0:td>
              </ns0:tr>
              <ns0:tr>
                <ns0:td align="center" border="l r t"><ns0:text color="#FF8000">You will be more likely to give up before the 30 minutes</ns0:text>, <ns0:text color="#0000FF">aap logo se jyada he</ns0:text>.</ns0:td>
                <ns0:td align="center" border="r t">7</ns0:td>
                <ns0:td align="center" border="r t">8</ns0:td>
                <ns0:td align="center" border="r t">9</ns0:td>
                <ns0:td align="center" border="r t">8</ns0:td>
              </ns0:tr>
              <ns0:tr>
                <ns0:td align="center" border="l r t"><ns0:text color="#0000FF">Shighra hi</ns0:text> maansingh <ns0:text color="#FF8000">british</ns0:text> <ns0:text color="#0000FF">ke saath saude baazi kar rha tha</ns0:text>.</ns0:td>
                <ns0:td align="center" border="r t">8</ns0:td>
                <ns0:td align="center" border="r t">10</ns0:td>
                <ns0:td align="center" border="r t">8</ns0:td>
                <ns0:td align="center" border="r t">8</ns0:td>
              </ns0:tr>
              <ns0:tr>
                <ns0:td align="center" border="l r t"><ns0:text color="#0000FF">par</ns0:text> <ns0:text color="#FF8000">there’s another way</ns0:text>, <ns0:text color="#0000FF">aur mai aapko bata kar ja rahi hun</ns0:text>.</ns0:td>
                <ns0:td align="center" border="r t">9</ns0:td>
                <ns0:td align="center" border="r t">10</ns0:td>
                <ns0:td align="center" border="r t">9</ns0:td>
                <ns0:td align="center" border="r t">9</ns0:td>
              </ns0:tr>
              <ns0:tr>
                <ns0:td align="center" border="b l r t">“<ns0:text color="#FF8000">Aren’t you a tiny bit</ns0:text> <ns0:text color="#0000FF">andhvishavaasi</ns0:text>?”</ns0:td>
                <ns0:td align="center" border="b r t">9</ns0:td>
                <ns0:td align="center" border="b r t">9</ns0:td>
                <ns0:td align="center" border="b r t">9</ns0:td>
                <ns0:td align="center" border="b r t">9</ns0:td>
              </ns0:tr>
            </ns0:tbody>
          </ns0:tabular></ns0:text></ns0:p>
      <ns0:toccaption><ns0:tag close=" "><ns0:text fontsize="90%">2</ns0:text></ns0:tag><ns0:text fontsize="90%">Example DCM and RA ratings provided by the three human annotators to the human-generated Hinglish sentences. We color code the tokens in the Hinglish sentence based on the language with the scheme: </ns0:text><ns0:text color="#FF8000" fontsize="90%">English</ns0:text><ns0:text fontsize="90%"> tokens with orange, </ns0:text><ns0:text color="#0000FF" fontsize="90%">Hindi</ns0:text><ns0:text fontsize="90%"> tokens with blue and language independent tokens with black color.</ns0:text></ns0:toccaption>
      <ns0:caption fontsize="90%"><ns0:tag close=": ">Table 2</ns0:tag>Example DCM and RA ratings provided by the three human annotators to the human-generated Hinglish sentences. We color code the tokens in the Hinglish sentence based on the language with the scheme: <ns0:text color="#FF8000">English</ns0:text> tokens with orange, <ns0:text color="#0000FF">Hindi</ns0:text> tokens with blue and language independent tokens with black color.</ns0:caption>
    </ns0:table>
  </ns0:section>
  <ns0:section inlist="toc" labels="LABEL:sec:_machine" xml:id="S3">
    <ns0:tags>
      <ns0:tag>3</ns0:tag>
      <ns0:tag role="refnum">3</ns0:tag>
      <ns0:tag role="typerefnum">§3</ns0:tag>
    </ns0:tags>
    <ns0:title><ns0:tag close=" ">3</ns0:tag>Machine-Generated Hinglish Text</ns0:title>
    <ns0:para xml:id="S3.p1">
      <ns0:p>In addition to the human-generated code-mixed text, we also generate the Hinglish sentence synthetically by following the Embedded-Matrix theory <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="joshi1982processing" separator="," yyseparator="," />]</ns0:cite>. We propose two rule-based Hinglish text generation systems leveraging the parallel monolingual English and Hindi sentences. In both systems, we use Hindi as the matrix language and English as the embedded language. The matrix language imparts structure to the code-mixed text with tokens embedded from embedded language. We also use several linguistic resources in both generation systems. These resources include:</ns0:p>
    </ns0:para>
    <ns0:para xml:id="S3.p2">
      <ns0:itemize xml:id="S3.I1">
        <ns0:item xml:id="S3.I1.i1">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">1st item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S3.I1.i1.p1">
            <ns0:p><ns0:text font="bold">English-Hindi Dictionary</ns0:text>: We curate 77,805 pairs of English words and the corresponding Hindi meanings from two sources<ns0:note mark="2" role="footnote" xml:id="footnote2"><ns0:tags>
                  <ns0:tag>2</ns0:tag>
                  <ns0:tag role="refnum">2</ns0:tag>
                  <ns0:tag role="typerefnum">footnote 2</ns0:tag>
                </ns0:tags><ns0:ref class="ltx_url" font="typewriter" href="http://www.cfilt.iitb.ac.in/~hdict/webinterface\_user/index.php">http://www.cfilt.iitb.ac.in/~hdict/webinterface\_user/index.php</ns0:ref></ns0:note><ns0:sup>,</ns0:sup><ns0:note mark="3" role="footnote" xml:id="footnote3"><ns0:tags>
                  <ns0:tag>3</ns0:tag>
                  <ns0:tag role="refnum">3</ns0:tag>
                  <ns0:tag role="typerefnum">footnote 3</ns0:tag>
                </ns0:tags><ns0:ref class="ltx_url" font="typewriter" href="https://jankaribook.com/verbs-list-verb-in-hindi/">https://jankaribook.com/verbs-list-verb-in-hindi/</ns0:ref></ns0:note> to construct an English-Hindi dictionary.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S3.I1.i2">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">2nd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S3.I1.i2.p1">
            <ns0:p><ns0:text font="bold">Cross-lingual Word Embedding</ns0:text>: We leverage multilingual word vectors for English and Hindi tokens. These word-vectors (<ns0:Math mode="inline" tex="dim=300" text="d * i * m = 300" xml:id="S3.I1.i2.p1.m1">
                <ns0:XMath>
                  <ns0:XMApp>
                    <ns0:XMTok meaning="equals" role="RELOP">=</ns0:XMTok>
                    <ns0:XMApp>
                      <ns0:XMTok meaning="times" role="MULOP">⁢</ns0:XMTok>
                      <ns0:XMTok font="italic" role="UNKNOWN">d</ns0:XMTok>
                      <ns0:XMTok font="italic" role="UNKNOWN">i</ns0:XMTok>
                      <ns0:XMTok font="italic" role="UNKNOWN">m</ns0:XMTok>
                    </ns0:XMApp>
                    <ns0:XMTok meaning="300" role="NUMBER">300</ns0:XMTok>
                  </ns0:XMApp>
                </ns0:XMath>
              </ns0:Math>) are generated from fastText’s multilingual pre-trained model <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="bojanowski2017enriching" separator="," yyseparator="," />]</ns0:cite>. We further map these vectors to a common space using VecMap <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="artetxe2018acl" separator="," yyseparator="," />]</ns0:cite>.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S3.I1.i3">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">3rd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S3.I1.i3.p1">
            <ns0:p><ns0:text font="bold">GIZA++</ns0:text>: GIZA++ <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="och03:asc" separator="," yyseparator="," />]</ns0:cite> learns the word alignment between the parallel sentences using an HMM based alignment model in an unsupervised manner. We train GIZA++ on <ns0:text font="italic">IIT-B corpus</ns0:text>.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S3.I1.i4">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">4th item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S3.I1.i4.p1">
            <ns0:p><ns0:text font="bold">Script Transliteration</ns0:text>: We transliterate the code-mixed sentences containing tokens in the Devanagari script to the Roman script <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="Ritwik2019" separator="," yyseparator="," />]</ns0:cite>.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S3.I1.i5">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">5th item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S3.I1.i5.p1">
            <ns0:p><ns0:text font="bold">YAKE</ns0:text>: We use YAKE <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="campos2020yake" separator="," yyseparator="," />]</ns0:cite>, an unsupervised automatic keyword extraction method, to extract the key-phrases from the monolingual English and Hindi sentences.</ns0:p>
          </ns0:para>
        </ns0:item>
      </ns0:itemize>
    </ns0:para>
    <ns0:para xml:id="S3.p3">
      <ns0:p>We further extend the English-Hindi dictionary by incorporating parallel sentences in the <ns0:text font="italic">IIT-B corpus</ns0:text>. We leverage VecMap’s shared representation to identify the closest word in English and the corresponding Hindi sentence. In addition, we also leverage GIZA++ to align the parallel sentences resulting in aligned English-Hindi tokens. Both of these steps extend the initial dictionary from 77,809 to 1,52,821 words and meaning pairs. We use this extended dictionary in the following two code-mixed text generation systems:</ns0:p>
      <ns0:itemize xml:id="S3.I2">
        <ns0:item xml:id="S3.I2.i1">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">1st item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S3.I2.i1.p1">
            <ns0:p><ns0:text font="bold">Word-aligned code-mixing (WAC)</ns0:text>: Here, we align the noun and adjective tokens between the parallel sentences using the extended dictionary. We replace all the aligned Hindi tokens with the corresponding English noun or adjective token and transliterate the resultant Hindi sentence to the Roman script. Figure <ns0:ref labelref="LABEL:fig:example_wac" /> demonstrates the example Hinglish text generated from the parallel monolingual English and Hindi sentences using the WAC procedure.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S3.I2.i2">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">2nd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S3.I2.i2.p1">
            <ns0:p><ns0:text font="bold">Phrase-aligned code-mixing (PAC)</ns0:text>: Here, we align the keyphrases of length up to three tokens between the parallel sentences. To identify the keyphrases, we use the YAKE tool. We replace all the aligned Hindi phrases with the corresponding longest matching English phrase and transliterate the resultant Hindi sentence to the Roman script. Figure <ns0:ref labelref="LABEL:fig:example_pac" /> demonstrates the example Hinglish text generated from the parallel monolingual English and Hindi sentences using the PAC procedure.</ns0:p>
          </ns0:para>
        </ns0:item>
      </ns0:itemize>
    </ns0:para>
    <ns0:figure inlist="lof" labels="LABEL:fig:example_wac" placement="!tbh" xml:id="S3.F2">
      <ns0:tags>
        <ns0:tag>Figure 2</ns0:tag>
        <ns0:tag role="refnum">2</ns0:tag>
        <ns0:tag role="typerefnum">Figure 2</ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="ltx_centering undefined">\includegraphics</ns0:ERROR>
      <ns0:p align="center">[width=1.0]word.png
</ns0:p>
      <ns0:toccaption class="ltx_centering"><ns0:tag close=" ">2</ns0:tag>An example Hinglish code-mixed sentence generated using WAC method.</ns0:toccaption>
      <ns0:caption class="ltx_centering"><ns0:tag close=": ">Figure 2</ns0:tag>An example Hinglish code-mixed sentence generated using WAC method.</ns0:caption>
    </ns0:figure>
    <ns0:figure inlist="lof" labels="LABEL:fig:example_pac" placement="!tbh" xml:id="S3.F3">
      <ns0:tags>
        <ns0:tag>Figure 3</ns0:tag>
        <ns0:tag role="refnum">3</ns0:tag>
        <ns0:tag role="typerefnum">Figure 3</ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="ltx_centering undefined">\includegraphics</ns0:ERROR>
      <ns0:p align="center">[width=1.0]phrase.png</ns0:p>
      <ns0:toccaption class="ltx_centering"><ns0:tag close=" ">3</ns0:tag>An example Hinglish code-mixed sentence generated using PAC method.</ns0:toccaption>
      <ns0:caption class="ltx_centering"><ns0:tag close=": ">Figure 3</ns0:tag>An example Hinglish code-mixed sentence generated using PAC method.</ns0:caption>
    </ns0:figure>
  </ns0:section>
  <ns0:section inlist="toc" xml:id="S4">
    <ns0:tags>
      <ns0:tag>4</ns0:tag>
      <ns0:tag role="refnum">4</ns0:tag>
      <ns0:tag role="typerefnum">§4</ns0:tag>
    </ns0:tags>
    <ns0:title><ns0:tag close=" ">4</ns0:tag>A Study on Evaluation of Code-Mixed Text Generation</ns0:title>
    <ns0:para xml:id="S4.p1">
      <ns0:p>In this section, we evaluate machine-generated code-mixed text. This study demonstrates severe limitations of five widely popular NLP metrics in evaluating code-mixed text generation performance. We leverage the following metrics: (i) Bilingual Evaluation Understudy Score (<ns0:text font="bold">BLEU</ns0:text>, <ns0:ERROR class="undefined">\citet</ns0:ERROR>papineni2002bleu), (ii)<ns0:text font="bold">NIST</ns0:text> <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="doddington2002automatic" separator="," yyseparator="," />]</ns0:cite>, (iii) BERTScore (<ns0:text font="bold">BS</ns0:text>, <ns0:ERROR class="undefined">\citet</ns0:ERROR>zhang2019bertscore), (iv) Word Error Rate (<ns0:text font="bold">WER</ns0:text>, <ns0:ERROR class="undefined">\citet</ns0:ERROR>levenshtein1966binary), and (v) Translator Error Rate (<ns0:text font="bold">TER</ns0:text>, <ns0:ERROR class="undefined">\citet</ns0:ERROR>snover2006study). Higher BLEU, NIST, or BS values and lower WER or TER values represent better generation performance. We conduct two experiments to evaluate the machine-generated Hinglish text:</ns0:p>
    </ns0:para>
    <ns0:para xml:id="S4.p2">
      <ns0:itemize xml:id="S4.I1">
        <ns0:item xml:id="S4.I1.i1">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">1st item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S4.I1.i1.p1">
            <ns0:p><ns0:text font="bold">Human evaluation</ns0:text>: First, we perform coarse-grained qualitative evaluation by randomly sampling 100 English-Hindi sentence pairs and corresponding WAC and PAC generated sentences. We employ two human evaluators<ns0:note mark="4" role="footnote" xml:id="footnote4"><ns0:tags>
                  <ns0:tag>4</ns0:tag>
                  <ns0:tag role="refnum">4</ns0:tag>
                  <ns0:tag role="typerefnum">footnote 4</ns0:tag>
                </ns0:tags>The evaluators are different from the annotators employed in Section <ns0:ref labelref="LABEL:sec:dataset" />).</ns0:note> who are proficient in English and Hindi languages to evaluate the quality of the generated sentences. We ask evaluators to provide one of the two labels — <ns0:text font="italic">Correct</ns0:text> and <ns0:text font="italic">Incorrect</ns0:text> — to each of the generated sentences. A sentence is marked <ns0:text font="italic">Correct</ns0:text> if it is following the semantics of the parallel sentences and has high readability. Table <ns0:ref labelref="LABEL:agreement" /> shows the annotator’s agreement on the randomly sampled set of sentences. The coarse-grained qualitative evaluation shows the correct generation in at least 50% of the cases. Table <ns0:ref labelref="LABEL:tab:_performance" /> shows the evaluation of the randomly sampled 100 sentences on five metrics. The results show the inefficacy of automatic evaluation metrics to capture the linguistic diversity of the generated code-mixed text due to spelling variations, omitted words, limited reference sentences, and informal writing style <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="srivastava2020phinc" separator="," yyseparator="," />]</ns0:cite>.
We further conduct a fine-grained qualitative human evaluation to measure the similarity between the machine-generated Hinglish sentences and the monolingual pair, the readability, and the grammatical correctness. We employ a new set of eight human evaluators to provide a rating between 1 (low quality) to 10 (high quality) to the PAC and WAC generated Hinglish sentences based on the following three parameters:</ns0:p>
            <ns0:itemize xml:id="S4.I1.i1.I1">
              <ns0:item xml:id="S4.I1.i1.I1.i1">
                <ns0:tags>
                  <ns0:tag><ns0:text font="bold">–</ns0:text></ns0:tag>
                  <ns0:tag role="typerefnum">1st item</ns0:tag>
                </ns0:tags>
                <ns0:para xml:id="S4.I1.i1.I1.i1.p1">
                  <ns0:p>The similarity between the generated Hinglish and the monolingual source sentences.</ns0:p>
                </ns0:para>
              </ns0:item>
              <ns0:item xml:id="S4.I1.i1.I1.i2">
                <ns0:tags>
                  <ns0:tag><ns0:text font="bold">–</ns0:text></ns0:tag>
                  <ns0:tag role="typerefnum">2nd item</ns0:tag>
                </ns0:tags>
                <ns0:para xml:id="S4.I1.i1.I1.i2.p1">
                  <ns0:p>The readability of the generated sentence.</ns0:p>
                </ns0:para>
              </ns0:item>
              <ns0:item xml:id="S4.I1.i1.I1.i3">
                <ns0:tags>
                  <ns0:tag><ns0:text font="bold">–</ns0:text></ns0:tag>
                  <ns0:tag role="typerefnum">3rd item</ns0:tag>
                </ns0:tags>
                <ns0:para xml:id="S4.I1.i1.I1.i3.p1">
                  <ns0:p>The grammatical correctness of the generated sentence.</ns0:p>
                </ns0:para>
              </ns0:item>
            </ns0:itemize>
          </ns0:para>
          <ns0:para xml:id="S4.I1.i1.p2">
            <ns0:p>Each generated sentence is rated by two human evaluators. All the evaluators have expert-level proficiency in the English and Hindi languages. Table <ns0:ref labelref="LABEL:fig:instance" /> shows ratings provided to a representative machine-generated Hinglish sentence. Figure <ns0:ref labelref="LABEL:fig:ratings" /> shows the distribution of the fine-grained human evaluation scores. For both procedures, the majority (WAC: 82.3% and PAC: 75.2%) of the sentences score in the range 6–9. None of the sentences received a rating score of 1. The results further corroborate our claim that automatic evaluation metrics undermine code-mixed text generation performance. Figure <ns0:ref labelref="LABEL:fig:disagreement" /> shows the distribution of the disagreement in the human evaluation of the generated sentences. We calculate the disagreement as to the absolute difference between the human evaluation scores. PAC-generated sentences are more prone to high disagreement (&gt;=5) in the human evaluation than WAC. This could be attributed to the fact that PAC-generated sentences are relatively less constrained, which leaves the evaluation to the expertise and interpretation of the Hinglish language by the annotators.</ns0:p>
          </ns0:para>
        </ns0:item>
      </ns0:itemize>
    </ns0:para>
    <ns0:table inlist="lot" labels="LABEL:agreement" placement="!tbh" xml:id="S4.T3">
      <ns0:tags>
        <ns0:tag><ns0:text fontsize="90%">Table 3</ns0:text></ns0:tag>
        <ns0:tag role="refnum"><ns0:text fontsize="90%">3</ns0:text></ns0:tag>
        <ns0:tag role="typerefnum"><ns0:text fontsize="90%">Table 3</ns0:text></ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="ltx_centering undefined">\resizebox</ns0:ERROR>
      <ns0:p align="center"><ns0:text fontsize="90%">!
<ns0:tabular class="ltx_guessed_headers" vattach="middle">
            <ns0:thead>
              <ns0:tr>
                <ns0:td border="l r t" rowspan="2" thead="column" />
                <ns0:td align="center" border="rr t" colspan="2" thead="column"><ns0:text font="bold">WAC</ns0:text></ns0:td>
                <ns0:td align="center" border="r t" colspan="2" thead="column"><ns0:text font="bold">PAC</ns0:text></ns0:td>
              </ns0:tr>
              <ns0:tr>
                <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">Agree</ns0:text></ns0:td>
                <ns0:td align="center" border="rr t" thead="column"><ns0:text font="bold">Disagree</ns0:text></ns0:td>
                <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">Agree</ns0:text></ns0:td>
                <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">Disagree</ns0:text></ns0:td>
              </ns0:tr>
            </ns0:thead>
            <ns0:tbody>
              <ns0:tr>
                <ns0:td align="center" border="l r t"><ns0:text font="bold italic">Correct</ns0:text></ns0:td>
                <ns0:td align="center" border="r t">56</ns0:td>
                <ns0:td align="center" border="b rr t" rowspan="2">22</ns0:td>
                <ns0:td align="center" border="r t">55</ns0:td>
                <ns0:td align="center" border="b r t" rowspan="2">40</ns0:td>
              </ns0:tr>
              <ns0:tr>
                <ns0:td align="center" border="b l r t"><ns0:text font="bold italic">Incorrect</ns0:text></ns0:td>
                <ns0:td align="center" border="b r t">22</ns0:td>
                <ns0:td align="center" border="b r t">5</ns0:td>
              </ns0:tr>
            </ns0:tbody>
          </ns0:tabular></ns0:text></ns0:p>
      <ns0:toccaption class="ltx_centering"><ns0:tag close=" "><ns0:text fontsize="90%">3</ns0:text></ns0:tag><ns0:text fontsize="90%">Annotator agreement on the randomly sampled sentences for WAC and PAC.</ns0:text></ns0:toccaption>
      <ns0:caption class="ltx_centering" fontsize="90%"><ns0:tag close=": ">Table 3</ns0:tag>Annotator agreement on the randomly sampled sentences for WAC and PAC.</ns0:caption>
    </ns0:table>
    <ns0:table inlist="lot" labels="LABEL:tab:_performance" placement="!tbh" xml:id="S4.T4">
      <ns0:tags>
        <ns0:tag><ns0:text fontsize="90%">Table 4</ns0:text></ns0:tag>
        <ns0:tag role="refnum"><ns0:text fontsize="90%">4</ns0:text></ns0:tag>
        <ns0:tag role="typerefnum"><ns0:text fontsize="90%">Table 4</ns0:text></ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="ltx_centering undefined">\resizebox</ns0:ERROR>
      <ns0:p align="center"><ns0:text fontsize="90%">!
<ns0:tabular class="ltx_guessed_headers" vattach="middle">
            <ns0:thead>
              <ns0:tr>
                <ns0:td border="l r t" thead="column" />
                <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">BLEU</ns0:text></ns0:td>
                <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">WER</ns0:text></ns0:td>
                <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">TER</ns0:text></ns0:td>
                <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">NIST</ns0:text></ns0:td>
                <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">BS</ns0:text></ns0:td>
              </ns0:tr>
            </ns0:thead>
            <ns0:tbody>
              <ns0:tr>
                <ns0:td align="center" border="l r t"><ns0:text font="bold">WAC</ns0:text></ns0:td>
                <ns0:td align="center" border="r t">0.1229</ns0:td>
                <ns0:td align="center" border="r t">0.8240</ns0:td>
                <ns0:td align="center" border="r t">0.7830</ns0:td>
                <ns0:td align="center" border="r t">2.2045</ns0:td>
                <ns0:td align="center" border="r t">0.857</ns0:td>
              </ns0:tr>
              <ns0:tr>
                <ns0:td align="center" border="b l r t"><ns0:text font="bold">PAC</ns0:text></ns0:td>
                <ns0:td align="center" border="b r t">0.1202</ns0:td>
                <ns0:td align="center" border="b r t">0.8228</ns0:td>
                <ns0:td align="center" border="b r t">0.7981</ns0:td>
                <ns0:td align="center" border="b r t">2.0497</ns0:td>
                <ns0:td align="center" border="b r t">0.857</ns0:td>
              </ns0:tr>
            </ns0:tbody>
          </ns0:tabular></ns0:text></ns0:p>
      <ns0:toccaption class="ltx_centering"><ns0:tag close=" "><ns0:text fontsize="90%">4</ns0:text></ns0:tag><ns0:text fontsize="90%">Automatic performance evaluation of the WAC and PAC procedures.</ns0:text></ns0:toccaption>
      <ns0:caption class="ltx_centering" fontsize="90%"><ns0:tag close=": ">Table 4</ns0:tag>Automatic performance evaluation of the WAC and PAC procedures.</ns0:caption>
    </ns0:table>
    <ns0:table inlist="lot" labels="LABEL:fig:instance" placement="!tbh" xml:id="S4.T5">
      <ns0:tags>
        <ns0:tag>Table 5</ns0:tag>
        <ns0:tag role="refnum">5</ns0:tag>
        <ns0:tag role="typerefnum">Table 5</ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="ltx_centering undefined">\includegraphics</ns0:ERROR>
      <ns0:p align="center">[width=1.0]instance.png</ns0:p>
      <ns0:toccaption class="ltx_centering"><ns0:tag close=" ">5</ns0:tag>Example human-generated and machine-generated Hinglish sentences from the dataset along with the source English and Hindi sentences. Two different human annotators rate the synthetic Hinglish sentences on the scale 1-10 (low-high quality</ns0:toccaption>
      <ns0:caption class="ltx_centering"><ns0:tag close=": ">Table 5</ns0:tag>Example human-generated and machine-generated Hinglish sentences from the dataset along with the source English and Hindi sentences. Two different human annotators rate the synthetic Hinglish sentences on the scale 1-10 (low-high quality</ns0:caption>
    </ns0:table>
    <ns0:figure inlist="lof" labels="LABEL:fig:ratings" placement="!tbh" xml:id="S4.F4">
      <ns0:tags>
        <ns0:tag>Figure 4</ns0:tag>
        <ns0:tag role="refnum">4</ns0:tag>
        <ns0:tag role="typerefnum">Figure 4</ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="ltx_centering undefined">\includegraphics</ns0:ERROR>
      <ns0:p align="center">[width=1.0]WAC_PAC.png</ns0:p>
      <ns0:toccaption class="ltx_centering"><ns0:tag close=" ">4</ns0:tag>Distribution of human evaluation of the generated Hinglish sentences using WAC and PAC.</ns0:toccaption>
      <ns0:caption class="ltx_centering"><ns0:tag close=": ">Figure 4</ns0:tag>Distribution of human evaluation of the generated Hinglish sentences using WAC and PAC.</ns0:caption>
    </ns0:figure>
    <ns0:figure inlist="lof" labels="LABEL:fig:disagreement" placement="!tbh" xml:id="S4.F5">
      <ns0:tags>
        <ns0:tag>Figure 5</ns0:tag>
        <ns0:tag role="refnum">5</ns0:tag>
        <ns0:tag role="typerefnum">Figure 5</ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="ltx_centering undefined">\includegraphics</ns0:ERROR>
      <ns0:p align="center">[width=1.0]disagreement.png</ns0:p>
      <ns0:toccaption class="ltx_centering"><ns0:tag close=" ">5</ns0:tag>Distribution of the disagreement between human evaluation of the generated Hinglish sentences using WAC and PAC.</ns0:toccaption>
      <ns0:caption class="ltx_centering"><ns0:tag close=": ">Figure 5</ns0:tag>Distribution of the disagreement between human evaluation of the generated Hinglish sentences using WAC and PAC.</ns0:caption>
    </ns0:figure>
    <ns0:para xml:id="S4.p3">
      <ns0:itemize xml:id="S4.I2">
        <ns0:item xml:id="S4.I2.i1">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">1st item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S4.I2.i1.p1">
            <ns0:p><ns0:text font="bold">Metric-based evaluation</ns0:text>: Next, we analyze the performance of five evaluation metrics on the machine-generated code-mixed text. Table <ns0:ref labelref="LABEL:tab:HS" /> shows the average scores for each of the metrics against the corresponding human-provided rating. As evident from the results, the metrics perform poorly on the code-mixed data. We further analyze the correlation<ns0:note mark="5" role="footnote" xml:id="footnote5"><ns0:tags>
                  <ns0:tag>5</ns0:tag>
                  <ns0:tag role="refnum">5</ns0:tag>
                  <ns0:tag role="typerefnum">footnote 5</ns0:tag>
                </ns0:tags>We experiment with Pearson Correlation Coefficient. The value ranges from -1 to 1.</ns0:note> of the metric scores with the human-provided ratings for both the text generation procedures. For this task, we divide the human ratings into three buckets:</ns0:p>
            <ns0:itemize xml:id="S4.I2.i1.I1">
              <ns0:item xml:id="S4.I2.i1.I1.i1">
                <ns0:tags>
                  <ns0:tag><ns0:text font="bold">–</ns0:text></ns0:tag>
                  <ns0:tag role="typerefnum">1st item</ns0:tag>
                </ns0:tags>
                <ns0:para xml:id="S4.I2.i1.I1.i1.p1">
                  <ns0:p>Bucket 1: Human rating between 2–10.</ns0:p>
                </ns0:para>
              </ns0:item>
              <ns0:item xml:id="S4.I2.i1.I1.i2">
                <ns0:tags>
                  <ns0:tag><ns0:text font="bold">–</ns0:text></ns0:tag>
                  <ns0:tag role="typerefnum">2nd item</ns0:tag>
                </ns0:tags>
                <ns0:para xml:id="S4.I2.i1.I1.i2.p1">
                  <ns0:p>Bucket 2: Human rating between 2–5.</ns0:p>
                </ns0:para>
              </ns0:item>
              <ns0:item xml:id="S4.I2.i1.I1.i3">
                <ns0:tags>
                  <ns0:tag><ns0:text font="bold">–</ns0:text></ns0:tag>
                  <ns0:tag role="typerefnum">3rd item</ns0:tag>
                </ns0:tags>
                <ns0:para xml:id="S4.I2.i1.I1.i3.p1">
                  <ns0:p>Bucket 3: Human rating between 6–10.</ns0:p>
                </ns0:para>
              </ns0:item>
            </ns0:itemize>
          </ns0:para>
          <ns0:para xml:id="S4.I2.i1.p2">
            <ns0:p>Table <ns0:ref labelref="LABEL:tab:corr" /> shows the results of the correlation between various metric scores and the human rating to the machine-generated code-mixed sentences using WAC and PAC procedures. Human rating for generated sentences in Bucket 3 is relatively highly correlated with the metric scores compared to Bucket 2. This behavior could be attributed to the fact that low-quality sentences are difficult to rate for the human annotators due to various reasons such as poor sentence structuring and many spelling mistakes.</ns0:p>
          </ns0:para>
        </ns0:item>
      </ns0:itemize>
    </ns0:para>
    <ns0:table inlist="lot" labels="LABEL:tab:HS" placement="!tbh" xml:id="S4.T6">
      <ns0:tags>
        <ns0:tag>Table 6</ns0:tag>
        <ns0:tag role="refnum">6</ns0:tag>
        <ns0:tag role="typerefnum">Table 6</ns0:tag>
      </ns0:tags>
      <ns0:tabular class="ltx_guessed_headers" vattach="middle">
        <ns0:thead>
          <ns0:tr>
            <ns0:td align="center" border="l r t" rowspan="2" thead="column row"><ns0:tabular vattach="middle">
                <ns0:tr>
                  <ns0:td align="center"><ns0:text font="bold">Human</ns0:text></ns0:td>
                </ns0:tr>
                <ns0:tr>
                  <ns0:td align="center"><ns0:text font="bold">Score</ns0:text></ns0:td>
                </ns0:tr>
              </ns0:tabular></ns0:td>
            <ns0:td align="center" border="r t" colspan="5" thead="column"><ns0:text font="bold">WAC</ns0:text></ns0:td>
            <ns0:td align="center" border="r t" colspan="5" thead="column"><ns0:text font="bold">PAC</ns0:text></ns0:td>
          </ns0:tr>
          <ns0:tr>
            <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">BLEU</ns0:text></ns0:td>
            <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">WER</ns0:text></ns0:td>
            <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">TER</ns0:text></ns0:td>
            <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">NIST</ns0:text></ns0:td>
            <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">BS</ns0:text></ns0:td>
            <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">BLEU</ns0:text></ns0:td>
            <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">WER</ns0:text></ns0:td>
            <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">TER</ns0:text></ns0:td>
            <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">NIST</ns0:text></ns0:td>
            <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">BS</ns0:text></ns0:td>
          </ns0:tr>
        </ns0:thead>
        <ns0:tbody>
          <ns0:tr>
            <ns0:td align="center" border="l r t" thead="row"><ns0:text font="bold">2</ns0:text></ns0:td>
            <ns0:td align="center" border="t">0.144</ns0:td>
            <ns0:td align="center" border="t">0.741</ns0:td>
            <ns0:td align="center" border="t">0.667</ns0:td>
            <ns0:td align="center" border="t">0.092</ns0:td>
            <ns0:td align="center" border="r t">0.851</ns0:td>
            <ns0:td align="center" border="t">0.126</ns0:td>
            <ns0:td align="center" border="t">0.672</ns0:td>
            <ns0:td align="center" border="t">0.698</ns0:td>
            <ns0:td align="center" border="t">0.176</ns0:td>
            <ns0:td align="center" border="r t">0.8603</ns0:td>
          </ns0:tr>
          <ns0:tr>
            <ns0:td align="center" border="l r" thead="row"><ns0:text font="bold">3</ns0:text></ns0:td>
            <ns0:td align="center">0.138</ns0:td>
            <ns0:td align="center">0.735</ns0:td>
            <ns0:td align="center">0.708</ns0:td>
            <ns0:td align="center">0.070</ns0:td>
            <ns0:td align="center" border="r">0.852</ns0:td>
            <ns0:td align="center">0.146</ns0:td>
            <ns0:td align="center">0.765</ns0:td>
            <ns0:td align="center">0.696</ns0:td>
            <ns0:td align="center">0.086</ns0:td>
            <ns0:td align="center" border="r">0.851</ns0:td>
          </ns0:tr>
          <ns0:tr>
            <ns0:td align="center" border="l r" thead="row"><ns0:text font="bold">4</ns0:text></ns0:td>
            <ns0:td align="center">0.133</ns0:td>
            <ns0:td align="center">0.695</ns0:td>
            <ns0:td align="center">0.666</ns0:td>
            <ns0:td align="center">0.103</ns0:td>
            <ns0:td align="center" border="r">0.849</ns0:td>
            <ns0:td align="center">0.143</ns0:td>
            <ns0:td align="center">0.744</ns0:td>
            <ns0:td align="center">0.703</ns0:td>
            <ns0:td align="center">0.100</ns0:td>
            <ns0:td align="center" border="r">0.8464</ns0:td>
          </ns0:tr>
          <ns0:tr>
            <ns0:td align="center" border="l r" thead="row"><ns0:text font="bold">5</ns0:text></ns0:td>
            <ns0:td align="center">0.135</ns0:td>
            <ns0:td align="center">0.711</ns0:td>
            <ns0:td align="center">0.681</ns0:td>
            <ns0:td align="center">0.110</ns0:td>
            <ns0:td align="center" border="r">0.853</ns0:td>
            <ns0:td align="center">0.153</ns0:td>
            <ns0:td align="center">0.726</ns0:td>
            <ns0:td align="center">0.680</ns0:td>
            <ns0:td align="center">0.114</ns0:td>
            <ns0:td align="center" border="r">0.8515</ns0:td>
          </ns0:tr>
          <ns0:tr>
            <ns0:td align="center" border="l r" thead="row"><ns0:text font="bold">6</ns0:text></ns0:td>
            <ns0:td align="center">0.141</ns0:td>
            <ns0:td align="center">0.697</ns0:td>
            <ns0:td align="center">0.670</ns0:td>
            <ns0:td align="center">0.102</ns0:td>
            <ns0:td align="center" border="r">0.852</ns0:td>
            <ns0:td align="center">0.164</ns0:td>
            <ns0:td align="center">0.689</ns0:td>
            <ns0:td align="center">0.646</ns0:td>
            <ns0:td align="center">0.124</ns0:td>
            <ns0:td align="center" border="r">0.8558</ns0:td>
          </ns0:tr>
          <ns0:tr>
            <ns0:td align="center" border="l r" thead="row"><ns0:text font="bold">7</ns0:text></ns0:td>
            <ns0:td align="center">0.161</ns0:td>
            <ns0:td align="center">0.663</ns0:td>
            <ns0:td align="center">0.630</ns0:td>
            <ns0:td align="center">0.111</ns0:td>
            <ns0:td align="center" border="r">0.856</ns0:td>
            <ns0:td align="center">0.176</ns0:td>
            <ns0:td align="center">0.661</ns0:td>
            <ns0:td align="center">0.618</ns0:td>
            <ns0:td align="center">0.121</ns0:td>
            <ns0:td align="center" border="r">0.8581</ns0:td>
          </ns0:tr>
          <ns0:tr>
            <ns0:td align="center" border="l r" thead="row"><ns0:text font="bold">8</ns0:text></ns0:td>
            <ns0:td align="center">0.177</ns0:td>
            <ns0:td align="center">0.621</ns0:td>
            <ns0:td align="center">0.589</ns0:td>
            <ns0:td align="center">0.127</ns0:td>
            <ns0:td align="center" border="r">0.859</ns0:td>
            <ns0:td align="center">0.177</ns0:td>
            <ns0:td align="center">0.639</ns0:td>
            <ns0:td align="center">0.605</ns0:td>
            <ns0:td align="center">0.128</ns0:td>
            <ns0:td align="center" border="r">0.8598</ns0:td>
          </ns0:tr>
          <ns0:tr>
            <ns0:td align="center" border="l r" thead="row"><ns0:text font="bold">9</ns0:text></ns0:td>
            <ns0:td align="center">0.212</ns0:td>
            <ns0:td align="center">0.571</ns0:td>
            <ns0:td align="center">0.538</ns0:td>
            <ns0:td align="center">0.150</ns0:td>
            <ns0:td align="center" border="r">0.865</ns0:td>
            <ns0:td align="center">0.184</ns0:td>
            <ns0:td align="center">0.614</ns0:td>
            <ns0:td align="center">0.590</ns0:td>
            <ns0:td align="center">0.129</ns0:td>
            <ns0:td align="center" border="r">0.8638</ns0:td>
          </ns0:tr>
          <ns0:tr>
            <ns0:td align="center" border="b l r" thead="row"><ns0:text font="bold">10</ns0:text></ns0:td>
            <ns0:td align="center" border="b">0.291</ns0:td>
            <ns0:td align="center" border="b">0.509</ns0:td>
            <ns0:td align="center" border="b">0.493</ns0:td>
            <ns0:td align="center" border="b">0.157</ns0:td>
            <ns0:td align="center" border="b r">0.878</ns0:td>
            <ns0:td align="center" border="b">0.242</ns0:td>
            <ns0:td align="center" border="b">0.551</ns0:td>
            <ns0:td align="center" border="b">0.543</ns0:td>
            <ns0:td align="center" border="b">0.146</ns0:td>
            <ns0:td align="center" border="b r">0.8731</ns0:td>
          </ns0:tr>
        </ns0:tbody>
      </ns0:tabular>
      <ns0:toccaption><ns0:tag close=" ">6</ns0:tag>Comparison of various metric scores with the human score for WAC and PAC.</ns0:toccaption>
      <ns0:caption><ns0:tag close=": ">Table 6</ns0:tag>Comparison of various metric scores with the human score for WAC and PAC.</ns0:caption>
    </ns0:table>
    <ns0:table inlist="lot" labels="LABEL:tab:corr" placement="!tbh" xml:id="S4.T7">
      <ns0:tags>
        <ns0:tag>Table 7</ns0:tag>
        <ns0:tag role="refnum">7</ns0:tag>
        <ns0:tag role="typerefnum">Table 7</ns0:tag>
      </ns0:tags>
      <ns0:ERROR class="undefined">\resizebox</ns0:ERROR>
      <ns0:p>!
<ns0:tabular class="ltx_guessed_headers" vattach="middle">
          <ns0:thead>
            <ns0:tr>
              <ns0:td border="l r t" rowspan="2" thead="column row" />
              <ns0:td align="center" border="r t" colspan="2" thead="column"><ns0:text font="bold">Bucket 1</ns0:text></ns0:td>
              <ns0:td align="center" border="r t" colspan="2" thead="column"><ns0:text font="bold">Bucket 2</ns0:text></ns0:td>
              <ns0:td align="center" border="r t" colspan="2" thead="column"><ns0:text font="bold">Bucket 3</ns0:text></ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">WAC</ns0:text></ns0:td>
              <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">PAC</ns0:text></ns0:td>
              <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">WAC</ns0:text></ns0:td>
              <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">PAC</ns0:text></ns0:td>
              <ns0:td align="center" border="t" thead="column"><ns0:text font="bold">WAC</ns0:text></ns0:td>
              <ns0:td align="center" border="r t" thead="column"><ns0:text font="bold">PAC</ns0:text></ns0:td>
            </ns0:tr>
          </ns0:thead>
          <ns0:tbody>
            <ns0:tr>
              <ns0:td align="center" border="l r t" thead="row"><ns0:text font="bold">BLEU</ns0:text></ns0:td>
              <ns0:td align="center" border="t">0.810</ns0:td>
              <ns0:td align="center" border="r t">0.910</ns0:td>
              <ns0:td align="center" border="t">-0.861</ns0:td>
              <ns0:td align="center" border="r t">0.878</ns0:td>
              <ns0:td align="center" border="t">0.941</ns0:td>
              <ns0:td align="center" border="r t">0.844</ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="l r" thead="row"><ns0:text font="bold">WER</ns0:text></ns0:td>
              <ns0:td align="center">-0.936</ns0:td>
              <ns0:td align="center" border="r">-0.822</ns0:td>
              <ns0:td align="center">-0.785</ns0:td>
              <ns0:td align="center" border="r">0.457</ns0:td>
              <ns0:td align="center">-0.993</ns0:td>
              <ns0:td align="center" border="r">-0.973</ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="l r" thead="row"><ns0:text font="bold">TER</ns0:text></ns0:td>
              <ns0:td align="center">-0.891</ns0:td>
              <ns0:td align="center" border="r">-0.963</ns0:td>
              <ns0:td align="center">0.000</ns0:td>
              <ns0:td align="center" border="r">-0.610</ns0:td>
              <ns0:td align="center">-0.998</ns0:td>
              <ns0:td align="center" border="r">-0.970</ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="l r" thead="row"><ns0:text font="bold">NIST</ns0:text></ns0:td>
              <ns0:td align="center">0.913</ns0:td>
              <ns0:td align="center" border="r">0.127</ns0:td>
              <ns0:td align="center">0.642</ns0:td>
              <ns0:td align="center" border="r">-0.559</ns0:td>
              <ns0:td align="center">0.986</ns0:td>
              <ns0:td align="center" border="r">0.846</ns0:td>
            </ns0:tr>
            <ns0:tr>
              <ns0:td align="center" border="b l r" thead="row"><ns0:text font="bold">BS</ns0:text></ns0:td>
              <ns0:td align="center" border="b">0.844</ns0:td>
              <ns0:td align="center" border="b r">0.710</ns0:td>
              <ns0:td align="center" border="b">0.227</ns0:td>
              <ns0:td align="center" border="b r">-0.689</ns0:td>
              <ns0:td align="center" border="b">0.953</ns0:td>
              <ns0:td align="center" border="b r">0.937</ns0:td>
            </ns0:tr>
          </ns0:tbody>
        </ns0:tabular></ns0:p>
      <ns0:toccaption><ns0:tag close=" ">7</ns0:tag>Comparison of correlation between evaluation metrics and human scores for WAC and PAC.</ns0:toccaption>
      <ns0:caption><ns0:tag close=": ">Table 7</ns0:tag>Comparison of correlation between evaluation metrics and human scores for WAC and PAC.</ns0:caption>
    </ns0:table>
  </ns0:section>
  <ns0:section inlist="toc" xml:id="S5">
    <ns0:tags>
      <ns0:tag>5</ns0:tag>
      <ns0:tag role="refnum">5</ns0:tag>
      <ns0:tag role="typerefnum">§5</ns0:tag>
    </ns0:tags>
    <ns0:title><ns0:tag close=" ">5</ns0:tag>Limitations and Opportunities</ns0:title>
    <ns0:para xml:id="S5.p1">
      <ns0:p>In this section, we present a discussion on the various inherent limitations associated with the proposed <ns0:text font="italic">HinGE</ns0:text> dataset. We also discuss the various opportunities for the computational linguistic community to build efficient systems and metrics for code-mixed languages. Some of the major limitations with the dataset are:</ns0:p>
    </ns0:para>
    <ns0:para xml:id="S5.p2">
      <ns0:itemize xml:id="S5.I1">
        <ns0:item xml:id="S5.I1.i1">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">1st item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S5.I1.i1.p1">
            <ns0:p>Due to the high time and cost associated with the human annotations, the number of samples in the dataset is limited. Because of a similar reason, the other code-mixing datasets <ns0:cite class="ltx_citemacro_cite">[<ns0:bibref bibrefs="srivastava2021challenges" separator="," yyseparator="," />]</ns0:cite> suffer from the scarcity of large-scale human annotations.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S5.I1.i2">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">2nd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S5.I1.i2.p1">
            <ns0:p>The IIT-B parallel corpus does not contain sentences mined from the social media platforms. This potentially reduces the noise in the generated sentences compared to the dataset previously compiled from the social media platforms for various tasks.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S5.I1.i3">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">3rd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S5.I1.i3.p1">
            <ns0:p>The annotators generating the code-mixed sentences were constrained only to include words from the parallel source sentences. This could potentially limit the observations compared to the real-world datasets collected from social media platforms that are more linguistically diverse due to the presence of a large number of multilingual speakers.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S5.I1.i4">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">4th item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S5.I1.i4.p1">
            <ns0:p>Due to the high annotation cost and time, the WAC and PAC generated sentences are only rated on a single scale encompassing multiple dimensions such as grammatical correctness, readability, etc.</ns0:p>
          </ns0:para>
        </ns0:item>
      </ns0:itemize>
    </ns0:para>
    <ns0:para xml:id="S5.p3">
      <ns0:p>Even with the presence of the above limitations, the <ns0:text font="italic">HinGE</ns0:text> dataset could be effectively used for various purposes such as:</ns0:p>
    </ns0:para>
    <ns0:para xml:id="S5.p4">
      <ns0:itemize xml:id="S5.I2">
        <ns0:item xml:id="S5.I2.i1">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">1st item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S5.I2.i1.p1">
            <ns0:p>The dataset could be effectively used in developing code-mixing text generation systems. Currently, the dataset supports only one code-mixed language, i.e., Hinglish, but it could be extended using various techniques such as weak supervision and active learning.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S5.I2.i2">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">2nd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S5.I2.i2.p1">
            <ns0:p>The machine-generated sentences and the corresponding human ratings will be useful in designing metrics and systems for the effective evaluation of various code-mixed NLG tasks. It could also be used to investigate the factors influencing the quality of the code-mixed text.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S5.I2.i3">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">3rd item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S5.I2.i3.p1">
            <ns0:p>The dataset could also be used in investigating the reasoning behind the disagreement in the human scores to the machine-generated sentences.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S5.I2.i4">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">4th item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S5.I2.i4.p1">
            <ns0:p>The code-mixed text (human or machine-generated) could be useful in the multitude of other code-mixing tasks such as language identification and POS tagging.</ns0:p>
          </ns0:para>
        </ns0:item>
        <ns0:item xml:id="S5.I2.i5">
          <ns0:tags>
            <ns0:tag>•</ns0:tag>
            <ns0:tag role="typerefnum">5th item</ns0:tag>
          </ns0:tags>
          <ns0:para xml:id="S5.I2.i5.p1">
            <ns0:p>With the recent thrust in code-mixed machine translation, the <ns0:text font="italic">HinGE</ns0:text> dataset would be extremely useful in designing and evaluating the machine-translation systems. The multiple code-mixed sentences corresponding to a given pair of parallel monolingual sentences would help to build robust translation systems.
</ns0:p>
          </ns0:para>
        </ns0:item>
      </ns0:itemize>
    </ns0:para>
  </ns0:section>
  <ns0:section inlist="toc" xml:id="S6">
    <ns0:tags>
      <ns0:tag>6</ns0:tag>
      <ns0:tag role="refnum">6</ns0:tag>
      <ns0:tag role="typerefnum">§6</ns0:tag>
    </ns0:tags>
    <ns0:title><ns0:tag close=" ">6</ns0:tag>Conclusion</ns0:title>
    <ns0:para xml:id="S6.p1">
      <ns0:p>In this paper, we present a high-quality dataset (<ns0:text font="italic">HinGE</ns0:text>) for the text-generation and evaluation task in the code-mixed Hinglish language. The code-mixed sentences in the <ns0:text font="italic">HinGE</ns0:text> dataset are generated by humans and rule-based algorithms. We demonstrate the poor evaluation capabilities of five widely popular metrics on the code-mixed data.
Along with the human-generated sentences, the machine-generated sentences (as described in Section <ns0:ref labelref="LABEL:sec:_machine" />) and the human ratings of these code-mixed sentences could facilitate building the highly scalable and robust evaluation metrics and strategies for the code-mixed text. The multiple human-generated sentences corresponding to a pair of parallel monolingual sentences will pave the way in designing natural language generation systems robust to adversaries and linguistic diversities such as spelling variation and matrix language.
</ns0:p>
    </ns0:para>
  </ns0:section>
  <ns0:bibliography bibstyle="acl_natbib" citestyle="numbers" files="custom" xml:id="bib">
    <ns0:title>References</ns0:title>
  <entry label="[{Artetxe et~al.(2018)Artetxe, Labaka, and Agirre}]{artetxe2018acl}&#10;Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.">
		<title>A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. </title>
		<Place>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics </Place>
		<volume> Long Papers</volume>
		<pages_start>789</pages_start>
		<pages_end>798</pages_end>
	</entry>
	<entry label="[{Banerjee et~al.(2018)Banerjee, Moghe, Arora, and&#10;  Khapra}]{banerjee2018dataset}&#10;Suman Banerjee, Nikita Moghe, Siddhartha Arora, and Mitesh~M Khapra. 2018.">
		<title>A dataset for building code-mixed goal oriented conversation systems. </title>
		<Place>Proceedings of the 27th International Conference on Computational Linguistics</Place>
		<pages_start>3766</pages_start>
		<pages_end>3780</pages_end>
	</entry>
	<entry label="[{Bojanowski et~al.(2017)Bojanowski, Grave, Joulin, and&#10;  Mikolov}]{bojanowski2017enriching}&#10;Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.">
		<title>Enriching word vectors with subword information. </title>
		<Place>Transactions of the Association for Computational Linguistics</Place>
		<pages_start>135</pages_start>
		<pages_end>146</pages_end>
	</entry>
	<entry label="[{Campos et~al.(2020)Campos, Mangaravite, Pasquali, Jorge, Nunes, and&#10;  Jatowt}]{campos2020yake}&#10;Ricardo Campos, V{\'\i}tor Mangaravite, Arian Pasquali, Al{\'\i}pio Jorge,&#10;  C{\'e}lia Nunes, and Adam Jatowt. 2020.">
		<title>Yake! keyword extraction from single documents using multiple local features. </title>
		<Place>Information Sciences</Place>
		<pages_start>257</pages_start>
		<pages_end>289</pages_end>
	</entry>
	<entry label="[{Doddington(2002)}]{doddington2002automatic}&#10;George Doddington. 2002.">
		<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. </title>
		<Place>Proceedings of the second international conference on Human Language Technology Research</Place>
		<pages_start>138</pages_start>
		<pages_end>145</pages_end>
	</entry>
	<entry label="[{Gao et~al.(2019)Gao, Feng, Liu, Hou, Pan, and Ma}]{Gao_Bert_GAN_2019}&#10;Yingying Gao, Junlan Feng, Ying Liu, Leijing Hou, Xin Pan, and Yong Ma. 2019.">
		<title>\href {https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2501.pdf} {Code-switching sentence generation by bert and generative adversarial networks}. </title>
		<Place>Proceedings of the 20th Annual Conference of the International Speech Communication Association, INTERSPEECH 2019</Place>
		<pages_start>3525</pages_start>
		<pages_end>3529</pages_end>
	</entry>
	<entry label="[{Gupta et~al.(2020)Gupta, Ekbal, and&#10;  Bhattacharyya}]{gupta-etal-2020-semi}&#10;Deepak Gupta, Asif Ekbal, and Pushpak Bhattacharyya. 2020.">
		<title>\href {https://doi.org/10.18653/v1/2020.findings-emnlp.206} {A semi-supervised approach to generate the code-mixed text using pre-trained encoder and transfer learning}. </title>
		<Place>Findings of the Association for Computational Linguistics: EMNLP 2020</Place>
		<pages_start>2267</pages_start>
		<pages_end>2280</pages_end>
	</entry>
	<entry label="[{Jain et~al.(2021)Jain, Prabhu, Vatsal, Ramena, and&#10;  Purre}]{dhruval_GCM_dependency_2021}&#10;Dhruval Jain, Arun~D Prabhu, Shubham Vatsal, Gopi Ramena, and Naresh Purre.&#10;  2021.">
		<title>\href {https://doi.org/10.1109/ICSC50631.2021.00030} {Codeswitched sentence creation using dependency parsing}. </title>
		<Place>2021 IEEE 15th International Conference on Semantic Computing </Place>
		<pages_start>124</pages_start>
		<pages_end>129</pages_end>
	</entry>
	<entry label="[{Joshi et~al.(2016)Joshi, Prabhu, Shrivastava, and&#10;  Varma}]{joshi2016towards}&#10;Aditya Joshi, Ameya Prabhu, Manish Shrivastava, and Vasudeva Varma. 2016.">
		<title>Towards sub-word level compositions for sentiment analysis of hindi-english code mixed text. </title>
		<Place>Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</Place>
		<pages_start>2482</pages_start>
		<pages_end>2491</pages_end>
	</entry>
	<entry label="[{Joshi(1982)}]{joshi1982processing}&#10;Aravind Joshi. 1982.">
		<title>Processing of sentences with intra-sentential code-switching. </title>
		<Place>Coling 1982: Proceedings of the Ninth International Conference on Computational Linguistics</Place>
		<pages_start />
		<pages_end />
	</entry>
	<entry label="[{Kunchukuttan et~al.(2018)Kunchukuttan, Mehta, and&#10;  Bhattacharyya}]{kunchukuttan2018iit}&#10;Anoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya. 2018.">
		<title>The iit bombay english-hindi parallel corpus. </title>
		<Place>Proceedings of the Eleventh International Conference on Language Resources and Evaluation </Place>
		<pages_start />
		<pages_end />
	</entry>
	<entry label="[{Lee et~al.(2019)Lee, Yue, and Li}]{Lee2019}&#10;Grandee Lee, Xianghu Yue, and Haizhou Li. 2019.">
		<title>\href {https://doi.org/10.21437/Interspeech.2019-1382} {{Linguistically Motivated Parallel Data Augmentation for Code-Switch Language Modeling}}. </title>
		<Place>Proc. Interspeech 2019</Place>
		<pages_start>3730</pages_start>
		<pages_end>3734</pages_end>
	</entry>
	<entry label="[{Levenshtein(1966)}]{levenshtein1966binary}&#10;Vladimir~I Levenshtein. 1966.">
		<title>Binary codes capable of correcting deletions, insertions, and reversals. </title>
		<Place>Soviet physics doklady</Place>
		<pages_start />
		<pages_end />
	</entry>
	<entry label="[{Mishra(2019)}]{Ritwik2019}&#10;Ritwik Mishra. 2019." />
	<entry label="[{Och and Ney(2003)}]{och03:asc}&#10;Franz~Josef Och and Hermann Ney. 2003.">
		<title>A systematic comparison of various statistical alignment models. </title>
		<Place>Computational Linguistics</Place>
		<pages_start />
		<pages_end />
	</entry>
	<entry label="[{Papineni et~al.(2002)Papineni, Roukos, Ward, and&#10;  Zhu}]{papineni2002bleu}&#10;Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.">
		<title>Bleu: a method for automatic evaluation of machine translation. </title>
		<Place>Proceedings of the 40th annual meeting on association for computational linguistics</Place>
		<pages_start>311</pages_start>
		<pages_end>318</pages_end>
	</entry>
	<entry label="[{Patro et~al.(2017)Patro, Samanta, Singh, Basu, Mukherjee, Choudhury,&#10;  and Mukherjee}]{patro-etal-2017-english}&#10;Jasabanta Patro, Bidisha Samanta, Saurabh Singh, Abhipsa Basu, Prithwish&#10;  Mukherjee, Monojit Choudhury, and Animesh Mukherjee. 2017.">
		<title>\href {https://doi.org/10.18653/v1/D17-1240} {All that is {E}nglish may be {H}indi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media}. </title>
		<Place>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</Place>
		<pages_start>2264</pages_start>
		<pages_end>2274</pages_end>
	</entry>
	<entry label="[{Patwa et~al.(2020)Patwa, Aguilar, Kar, Pandey, Srinivas, Gamb{\&quot;a}ck,&#10;  Chakraborty, Solorio, and Das}]{patwa2020semeval}&#10;Parth Patwa, Gustavo Aguilar, Sudipta Kar, Suraj Pandey, PYKL Srinivas,&#10;  Bj{\&quot;o}rn Gamb{\&quot;a}ck, Tanmoy Chakraborty, Thamar Solorio, and Amitava Das.&#10;  2020.">
		<title>Semeval-2020 task 9: Overview of sentiment analysis of code-mixed tweets. </title>
		<Place>Proceedings of the Fourteenth Workshop on Semantic Evaluation</Place>
		<pages_start>774</pages_start>
		<pages_end>790</pages_end>
	</entry>
	<entry label="[{Pratapa et~al.(2018)Pratapa, Bhat, Choudhury, Sitaram, Dandapat, and&#10;  Bali}]{pratapa-etal-2018-language}&#10;Adithya Pratapa, Gayatri Bhat, Monojit Choudhury, Sunayana Sitaram, Sandipan&#10;  Dandapat, and Kalika Bali. 2018.">
		<title>\href {https://doi.org/10.18653/v1/P18-1143} {Language modeling for code-mixing: The role of linguistic theory based synthetic data}. </title>
		<Place>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics </Place>
		<volume> Long Papers</volume>
		<pages_start>1543</pages_start>
		<pages_end>1553</pages_end>
	</entry>
	<entry label="[{Ramanarayanan et~al.(2019)Ramanarayanan, Pugh, Qian, and&#10;  Suendermann-Oeft}]{ramanarayanan2019automatic}&#10;Vikram Ramanarayanan, Robert Pugh, Yao Qian, and David Suendermann-Oeft. 2019.">
		<title>Automatic turn-level language identification for code-switched spanish--english dialog. </title>
		<Place>9th International Workshop on Spoken Dialogue System Technology</Place>
		<pages_start>51</pages_start>
		<pages_end>61</pages_end>
	</entry>
	<entry label="[{Rijhwani et~al.(2017)Rijhwani, Sequiera, Choudhury, Bali, and&#10;  Maddila}]{rijhwani-etal-2017-estimating}&#10;Shruti Rijhwani, Royal Sequiera, Monojit Choudhury, Kalika Bali, and&#10;  Chandra~Shekhar Maddila. 2017.">
		<title>\href {https://doi.org/10.18653/v1/P17-1180} {Estimating code-switching on {T}witter with a novel generalized word-level language detection technique}. </title>
		<Place>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics </Place>
		<volume> Long Papers</volume>
		<pages_start>1971</pages_start>
		<pages_end>1982</pages_end>
	</entry>
	<entry label="[{Shekhar et~al.(2020)Shekhar, Sharma, and Beg}]{shekhar2020language}&#10;Shashi Shekhar, Dilip~Kumar Sharma, and MM~Sufyan Beg. 2020.">
		<title>Language identification framework in code-mixed social media text based on quantum lstm—the word belongs to which language? </title>
		<Place>Modern Physics Letters B</Place>
		<pages_start />
		<pages_end />
	</entry>
	<entry label="[{Singh et~al.(2018{\natexlab{a}})Singh, Sen, and&#10;  Kumaraguru}]{singh2018language}&#10;Kushagra Singh, Indira Sen, and Ponnurangam Kumaraguru. 2018{\natexlab{a}}." />
	<entry label="[{Singh et~al.(2018{\natexlab{b}})Singh, Sen, and&#10;  Kumaraguru}]{singh2018twitter}&#10;Kushagra Singh, Indira Sen, and Ponnurangam Kumaraguru. 2018{\natexlab{b}}." />
	<entry label="[{Snover et~al.(2006)Snover, Dorr, Schwartz, Micciulla, and&#10;  Makhoul}]{snover2006study}&#10;Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John&#10;  Makhoul. 2006.">
		<title>A study of translation edit rate with targeted human annotation. </title>
		<Place>Proceedings of association for machine translation in the Americas</Place>
		<pages_start />
		<pages_end />
	</entry>
	<entry label="[{Solorio et~al.(2014)Solorio, Blair, Maharjan, Bethard, Diab, Ghoneim,&#10;  Hawwari, AlGhamdi, Hirschberg, Chang, and Fung}]{solorio-etal-2014-overview}&#10;Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steven Bethard, Mona Diab,&#10;  Mahmoud Ghoneim, Abdelati Hawwari, Fahad AlGhamdi, Julia Hirschberg, Alison&#10;  Chang, and Pascale Fung. 2014.">
		<title>\href {https://doi.org/10.3115/v1/W14-3907} {Overview for the first shared task on language identification in code-switched data}. </title>
		<Place>Proceedings of the First Workshop on Computational Approaches to Code Switching</Place>
		<pages_start>62</pages_start>
		<pages_end>72</pages_end>
	</entry>
	<entry label="[{Srivastava and Singh(2020)}]{srivastava2020phinc}&#10;Vivek Srivastava and Mayank Singh. 2020.">
		<title>Phinc: a parallel hinglish social media code-mixed corpus for machine translation. </title>
		<Place>arXiv preprint arXiv:2004.09447</Place>
		<pages_start />
		<pages_end />
	</entry>
	<entry label="[{Srivastava and Singh(2021)}]{srivastava2021challenges}&#10;Vivek Srivastava and Mayank Singh. 2021." />
	<entry label="[{Swami et~al.(2018)Swami, Khandelwal, Singh, Akhtar, and&#10;  Shrivastava}]{swami2018corpus}&#10;Sahil Swami, Ankush Khandelwal, Vinay Singh, Syed~Sarfaraz Akhtar, and Manish&#10;  Shrivastava. 2018.">
		<title>A corpus of english-hindi code-mixed tweets for sarcasm detection. </title>
		<Place>arXiv preprint arXiv:1805.11869</Place>
		<pages_start />
		<pages_end />
	</entry>
	<entry label="[{Utsav et~al.(2020)Utsav, Kabaria, Vajpeyi, Mina, and&#10;  Srivastava}]{10.1145/3371158.3371226}&#10;Jethva Utsav, Dhaiwat Kabaria, Ribhu Vajpeyi, Mohit Mina, and Vivek Srivastava.&#10;  2020." />
	<entry label="[{Vyas et~al.(2014)Vyas, Gella, Sharma, Bali, and&#10;  Choudhury}]{vyas2014pos}&#10;Yogarshi Vyas, Spandana Gella, Jatin Sharma, Kalika Bali, and Monojit&#10;  Choudhury. 2014.">
		<title>Pos tagging of english-hindi code-mixed social media content. </title>
		<Place>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing </Place>
		<pages_start>974</pages_start>
		<pages_end>979</pages_end>
	</entry>
	<entry label="[{Winata et~al.(2018)Winata, {Madotto}, {Wu}, and&#10;  {Fung}}]{2018arXiv181010254I}&#10;Genta~Indra Winata, Andrea {Madotto}, Chien-Sheng {Wu}, and Pascale {Fung}.&#10;  2018." />
	<entry label="[{Winata et~al.(2019)Winata, Madotto, Wu, and&#10;  Fung}]{winata-etal-2019-code}&#10;Genta~Indra Winata, Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2019.">
		<title>\href {https://doi.org/10.18653/v1/K19-1026} {Code-switched language models using neural based synthetic data from parallel sentences}. </title>
		<Place>Proceedings of the 23rd Conference on Computational Natural Language Learning </Place>
		<pages_start>271</pages_start>
		<pages_end>280</pages_end>
	</entry>
	<entry label="[{Zhang et~al.(2019{\natexlab{a}})Zhang, Zhang, and&#10;  Fu}]{zhang2019cross}&#10;Meishan Zhang, Yue Zhang, and Guohong Fu. 2019{\natexlab{a}}." />
	<entry label="[{Zhang et~al.(2019{\natexlab{b}})Zhang, Kishore, Wu, Weinberger, and&#10;  Artzi}]{zhang2019bertscore}&#10;Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.&#10;  2019{\natexlab{b}}." />
</ns0:bibliography>
</ns0:document>